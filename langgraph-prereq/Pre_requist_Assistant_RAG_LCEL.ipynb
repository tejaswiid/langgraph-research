{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langgraph-prerequist'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGCHAIN_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"]= GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=LANGCHAIN_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")'''\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "llm=ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Sunny. I am Gemini, a multi-modal AI model, developed by Google. How can I assist you today?\n",
      "Hi Sunny, it's nice to meet you! How can I help you today?\n",
      "I do not have access to your personal information, including your name.\n",
      "I do not have access to your personal information, so I cannot tell you your name.\n",
      "goodbye take care yourself\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question=input(\"type your question. if you want to quit the chat write quit\")\n",
    "    if question !=\"quit\":\n",
    "        print(llm.invoke(question).content)\n",
    "    else:\n",
    "        print(\"goodbye take care yourself\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"firstchat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_memory=RunnableWithMessageHistory(llm,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, sunnysavita! It's nice to meet you. My name is Gemini. I am a multi-modal AI model, developed by Google. I am designed to assist and provide information to the best of my abilities. Is there anything I can help you with today?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke((\"Hi! I'm sunnysavita\"),config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is sunnysavita. You told me this when you introduced yourself. Is there anything else I can help you with today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke((\"tell me what is my name?\"),config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'firstchat': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm sunnysavita\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, sunnysavita! It's nice to meet you. My name is Gemini. I am a multi-modal AI model, developed by Google. I am designed to assist and provide information to the best of my abilities. Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-fc4908ce-f4a1-4f0f-aab4-1914f82593b8-0', usage_metadata={'input_tokens': 10, 'output_tokens': 58, 'total_tokens': 68}), HumanMessage(content='tell me what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is sunnysavita. You told me this when you introduced yourself. Is there anything else I can help you with today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-4f901433-80d6-4fdb-9958-3285c7144295-0', usage_metadata={'input_tokens': 77, 'output_tokens': 28, 'total_tokens': 105})])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough , RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the txt files from source directory\n",
    "\n",
    "loader = DirectoryLoader('../data', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n",
    "\n",
    "###  BGE Embddings\n",
    "\n",
    "'''from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "'''\n",
    "\n",
    "### Creating Retriever using Vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are three important points about Llama3:\n",
      "\n",
      "1. **Llama3 is a language model:** It is a large language model developed by Meta AI.\n",
      "2. **It was released in April 2024:** Llama3 was released in April 2024, according to the provided context.\n",
      "3. **It is used by various services:** Both Notion AI and Gemini use a Llama3 model, as stated in the context.\n"
     ]
    }
   ],
   "source": [
    "question =\"what is llama3? can you highlight 3 important points?\"\n",
    "print(retrieval_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start with Tools and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'query to look up on wikipedia',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval augmented generation (RAG) is a type of generative artificial intelligence that has information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information in preference to information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \n",
      "Use cases include providing chatbot access to internal company data, or giving factual information only from an authoritative source.\n",
      "\n",
      "\n",
      "\n",
      "Page: DataStax\n",
      "Summary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tool.run({\"query\": \"langchain\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2=YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youtube_search'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=V9tJCQoBakA&pp=ygUMc3Vubnkgc2F2aXRh', 'https://www.youtube.com/watch?v=6GnsndNL5u4&pp=ygUMc3Vubnkgc2F2aXRh']\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.run(\"sunny savita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool3=TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Burning Man\\nSummary: Burning Man is a week-long large-scale desert event focused on \"community, art, self-expression, and self-reliance\" held annually in the western United States. The event\\'s name comes from its culminating ceremony: the symbolic burning of a large wooden effigy, referred to as the Man, that occurs on the penultimate night, the Saturday evening before Labor Day. Since 1991, the event has been at Black Rock City in northwestern Nevada, a temporary city erected in the Black Rock Desert about 100 miles (160 km) north-northeast of Reno. According to Burning Man co-founder Larry Harvey in 2004, the event is guided by ten stated principles: radical inclusion, gifting, decommodification, radical self-reliance, radical self-expression, communal effort, civic responsibility, leaving no trace, participation, and immediacy.\\nBurning Man features no headliners or scheduled performers; instead, participants create all the art, activities, and events. Artwork includes experimental and interactive sculptures, buildings, performances, and art cars, among other media. These contributions are inspired by a theme chosen annually by the Burning Man Project. The event has been called \"countercultural revelry\" and described by its organizers as an \"excuse to party in the desert\". However, NPR said of Burning Man in 2019, \"Once considered an underground gathering for bohemians and free spirits of all stripes, Burning Man has since evolved into a destination for social media influencers, celebrities and the Silicon Valley elite.\"\\n\\nBurning Man originated on June 22, 1986, on Baker Beach in San Francisco as a small function organized by Larry Harvey and Jerry James, the builders of the first Man. It has since been held annually, spanning the nine days leading up to and including Labor Day. Over the event\\'s history, attendance has generally increased. In 2019, 78,850 people participated.\\nBurning Man is organized by the Burning Man Project, a nonprofit organization that, in 2013, succeeded Black Rock City LLC, a for-profit limited liability company. Black Rock City LLC was formed in 1999 to represent the event\\'s organizers and is now considered a subsidiary of the nonprofit organization. The Burning Man Project endorses multiple smaller regional events guided by the Burning Man principles, in the United States and internationally.\\nThe film Stalker by Tarkovsky heavily influenced the Cacophony Society, which began in 1986 in the San Francisco Bay Area and which organized \"Zone Trips\" for participants.\\nThe first burning of a wooden, symbolic man at Black Rock Desert, Nevada, occurred on \"Zone Trip Number 4\" in 1990. This occasion evolved into an enormous annual festival of arts, music, culture called Burning Man.\\n\\nPage: Resistance (video game series)\\nSummary: Resistance is a series of first-person shooter and third-person shooter video games developed by Insomniac Games and published by Sony Computer Entertainment for the PlayStation 3, PlayStation Portable and PlayStation Vita video game consoles. The series takes place in an alternate history 1950s, in which an alien civilization known as the Chimera have invaded and conquered Earth, expanding their armies by capturing humans and transforming them into monster-like super soldiers to fight for them. The player takes the role of one of the remaining human armed forces as they fight against the Chimera invasion. The series is noted for its use of both conventional and futuristic weaponry, reflecting Insomniac\\'s previous work on the Ratchet & Clank series.\\n\\n\\n\\nPage: Destroyed in Seconds\\nSummary: Destroyed in Seconds is an American television series that premiered on Discovery Channel on August 21, 2008. \\nHosted by Ron Pitts, it features video segments of various things being destroyed fairly quickly (hence, \"in seconds\") such as planes crashing, explosions, sinkholes, boats crashing, fires, race car incidents, floods, factories, etc. The nature of the show closely resembles Real TV. The'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke({\"query\": \"What happened in the latest burning man floods\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools \n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=load_tools([\"wikipedia\"],llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=initialize_agent(tool,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of india\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: India GDP\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: India GDP\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: India GDP\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: GDP of India\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: [wikipedia]\n",
      "Action Input: Indian GDP\u001b[0m\n",
      "Observation: [wikipedia] is not a valid tool, try one of [wikipedia].\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is current GDP of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
